type: LLamaForCausalLMPool
# each model should have a name and a path, and the model is loaded from the path
# this is equivalent to `AutoModelForCausalLM.from_pretrained(path)`
models:
  # Example:
  # - name:
  #   path:
  #   model_kwargs:
  #     - cache_dir: null
  - name: _pretrained_
    path: meta-llama/Meta-Llama-3-8B
  - name: expert_1
    path: meta-llama/Meta-Llama-3-8B
  - name: expert_2
    path: meta-llama/Meta-Llama-3-8B-Instruct

dtype: float16 # this will be overrided by the `torch_dtype` in model_kwargs
model_kwargs: null
